{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 飞桨常规赛：PALM病理性近视预测 - 5月第一名方案\n",
    "第一次参加常规赛进入了前三，有点小激动。虽然也没啥技巧，只会跑跑跑，但还是作为记录，说不定能给特别缘分的朋友一点启发。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 0. 赛题介绍\n",
    "常规赛：PALM病理性近视预测由ISBI2019 PALM眼科挑战赛赛题再现，其中病理性近视预测的任务旨在对眼科图像进行判断，获得该眼为病理性近视的概率。\n",
    "\n",
    "数据集由中山大学中山眼科中心提供800张带病理性近视分类标注的眼底彩照供选手训练模型，另提供400张带标注数据供平台进行模型测试。图像分辨率为1444×1444，或2124×2056。\n",
    "\n",
    "评价指标为AUC (Area Under Curve)，即ROC (Receiver operating characteristic) 曲线与坐标轴形成的面积。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/64afb1df4595408088e547b616423f447e5761ae13224ca4be559b0c598d598b)\n",
    "\n",
    "比赛链接: [常规赛：PALM病理性近视预测](https://aistudio.baidu.com/aistudio/competition/detail/85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. 包准备\n",
    "既然是分类任务，首先想到的是PaddleClas。但是我忽然想到了大佬们搞的ppim。因为医疗影像我认为有重要的关注的地方，和遥感图像不太类似，注意力应该能取得较好的效果。听闻大佬们的ppim复现了很新的注意力网络，而且和源代码相比效果很好，所以决定试试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! pip -q install ppim -i https://pypi.python.org/pypi\r\n",
    "# ! git -q clone https://github.com/AgentMaker/Paddle-Image-Models.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. 数据准备\n",
    "### 2.1解压数据集\n",
    "这个没啥好写的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ! unzip -oq /home/aistudio/data/data85133/常规赛：PALM病理性近视预测.zip\r\n",
    "# ! rm -rf __MACOSX\r\n",
    "# ! mv 常规赛：PALM病理性近视预测 PLAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.2 配置数据集\n",
    "- 因为数据中本身就有了这个图像名和标签，我们就不用生成数据列表了。直接继承io中的Dataset，用于读取数据。因为与开始说数据的大小有两种分辨率，而且贼大，但是又不敢放的太小损失太多细节，所以这里都放到了1120X1120。\n",
    "- 划分的比列为0.9，图像增强只有简单的色彩和水平翻转。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def convert_to_list(value, n, name, dtype=np.int):\n"
     ]
    }
   ],
   "source": [
    "import os\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import paddle\r\n",
    "import paddle.vision.transforms as T\r\n",
    "from paddle.io import Dataset\r\n",
    "from PIL import Image\r\n",
    "\r\n",
    "class PLAMDatas(Dataset):\r\n",
    "    def __init__(self, data_path, class_xls, mode='train', transforms=None):\r\n",
    "        super(PLAMDatas, self).__init__()\r\n",
    "        self.data_path = data_path\r\n",
    "        self.name_label = (pd.read_excel(class_xls)).values\r\n",
    "        lens = len(self.name_label)\r\n",
    "        if mode == 'train':\r\n",
    "            self.name_label = self.name_label[:int(0.9*lens)]\r\n",
    "        else:\r\n",
    "            self.name_label = self.name_label[int(0.9*lens):]\r\n",
    "        self.transforms = transforms\r\n",
    "        \r\n",
    "    def __getitem__(self, index):\r\n",
    "        name, label = self.name_label[index]\r\n",
    "        data_path = os.path.join(self.data_path, name)\r\n",
    "        data = np.asarray(Image.open(data_path).convert('RGB'))\r\n",
    "        if self.transforms is not None:\r\n",
    "            data = self.transforms(data)\r\n",
    "        data = data.astype('float32')\r\n",
    "        label = np.array(int(label)).astype('int64')\r\n",
    "        return data, label\r\n",
    "        \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.name_label)\r\n",
    "\r\n",
    "# 配置数据增广\r\n",
    "train_transforms = T.Compose([\r\n",
    "    T.Resize((1120, 1120), interpolation='bicubic'),\r\n",
    "    T.ColorJitter(0.1, 0.1, 0.1, 0.1),\r\n",
    "    T.RandomHorizontalFlip(),\r\n",
    "   \tT.ToTensor()\r\n",
    "])\r\n",
    "\r\n",
    "val_transforms = T.Compose([\r\n",
    "    T.Resize((1120, 1120), interpolation='bicubic'),\r\n",
    "    T.ToTensor()\r\n",
    "])\r\n",
    "\r\n",
    "# 配置数据集\r\n",
    "train_dataset = PLAMDatas(data_path='PLAM/Train/fundus_image', class_xls='PLAM/Train/Classification.xlsx', mode='train', transforms=train_transforms)\r\n",
    "val_dataset = PLAMDatas(data_path='PLAM/Train/fundus_image', class_xls='PLAM/Train/Classification.xlsx', mode='test', transforms=val_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "这里也是输出测试一下，看看数据读取有没有什么问题。避免后面报一堆错不知道哪儿去找问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720 80\n",
      "[3, 1120, 1120] 0\n",
      "[3, 1120, 1120] 0\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset), len(val_dataset))\r\n",
    "for img, lab in train_dataset:\r\n",
    "    print(img.shape, lab)\r\n",
    "    break\r\n",
    "for img ,lab in val_dataset:\r\n",
    "    print(img.shape, lab)\r\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. 模型训练\n",
    "### 3.1 模型准备\n",
    "这里最开始直接采用的是deit_b_distilled_384之类的，然而问题就是没注意到大佬已经规定好了图像的大小了，搞了几次没不对，一度准备放弃改用PaddleClas的模型，后来终于幡然醒悟。384或者224对这个任务来说太小了，没办法，用了基础的DistilledVisionTransformer，自己说用1120的。但是这样就没有预训练的参数了，只能自己跑了。patch_size也改了一下，太小的话空间占用多，运算慢，而且不太好。summary看一下，总算没问题了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      " Layer (type)       Input Shape          Output Shape         Param #    \n",
      "===========================================================================\n",
      "   Conv2D-1     [[1, 3, 1120, 1120]]   [1, 768, 17, 17]      9,437,952   \n",
      " PatchEmbed-1   [[1, 3, 1120, 1120]]    [1, 289, 768]            0       \n",
      "   Dropout-1      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  LayerNorm-1     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-1       [[1, 291, 768]]       [1, 291, 2304]       1,769,472   \n",
      "   Dropout-2    [[1, 12, 291, 291]]   [1, 12, 291, 291]          0       \n",
      "   Linear-2       [[1, 291, 768]]       [1, 291, 768]         590,592    \n",
      "   Dropout-3      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  Attention-1     [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  Identity-1      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  LayerNorm-2     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-3       [[1, 291, 768]]       [1, 291, 3072]       2,362,368   \n",
      "    GELU-1        [[1, 291, 3072]]      [1, 291, 3072]           0       \n",
      "   Dropout-4      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "   Linear-4       [[1, 291, 3072]]      [1, 291, 768]        2,360,064   \n",
      "     Mlp-1        [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "    Block-1       [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  LayerNorm-3     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-5       [[1, 291, 768]]       [1, 291, 2304]       1,769,472   \n",
      "   Dropout-5    [[1, 12, 291, 291]]   [1, 12, 291, 291]          0       \n",
      "   Linear-6       [[1, 291, 768]]       [1, 291, 768]         590,592    \n",
      "   Dropout-6      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  Attention-2     [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  Identity-2      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  LayerNorm-4     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-7       [[1, 291, 768]]       [1, 291, 3072]       2,362,368   \n",
      "    GELU-2        [[1, 291, 3072]]      [1, 291, 3072]           0       \n",
      "   Dropout-7      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "   Linear-8       [[1, 291, 3072]]      [1, 291, 768]        2,360,064   \n",
      "     Mlp-2        [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "    Block-2       [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  LayerNorm-5     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-9       [[1, 291, 768]]       [1, 291, 2304]       1,769,472   \n",
      "   Dropout-8    [[1, 12, 291, 291]]   [1, 12, 291, 291]          0       \n",
      "   Linear-10      [[1, 291, 768]]       [1, 291, 768]         590,592    \n",
      "   Dropout-9      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  Attention-3     [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  Identity-3      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  LayerNorm-6     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-11      [[1, 291, 768]]       [1, 291, 3072]       2,362,368   \n",
      "    GELU-3        [[1, 291, 3072]]      [1, 291, 3072]           0       \n",
      "  Dropout-10      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "   Linear-12      [[1, 291, 3072]]      [1, 291, 768]        2,360,064   \n",
      "     Mlp-3        [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "    Block-3       [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  LayerNorm-7     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-13      [[1, 291, 768]]       [1, 291, 2304]       1,769,472   \n",
      "  Dropout-11    [[1, 12, 291, 291]]   [1, 12, 291, 291]          0       \n",
      "   Linear-14      [[1, 291, 768]]       [1, 291, 768]         590,592    \n",
      "  Dropout-12      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  Attention-4     [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  Identity-4      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  LayerNorm-8     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-15      [[1, 291, 768]]       [1, 291, 3072]       2,362,368   \n",
      "    GELU-4        [[1, 291, 3072]]      [1, 291, 3072]           0       \n",
      "  Dropout-13      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "   Linear-16      [[1, 291, 3072]]      [1, 291, 768]        2,360,064   \n",
      "     Mlp-4        [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "    Block-4       [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  LayerNorm-9     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-17      [[1, 291, 768]]       [1, 291, 2304]       1,769,472   \n",
      "  Dropout-14    [[1, 12, 291, 291]]   [1, 12, 291, 291]          0       \n",
      "   Linear-18      [[1, 291, 768]]       [1, 291, 768]         590,592    \n",
      "  Dropout-15      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  Attention-5     [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  Identity-5      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      " LayerNorm-10     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-19      [[1, 291, 768]]       [1, 291, 3072]       2,362,368   \n",
      "    GELU-5        [[1, 291, 3072]]      [1, 291, 3072]           0       \n",
      "  Dropout-16      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "   Linear-20      [[1, 291, 3072]]      [1, 291, 768]        2,360,064   \n",
      "     Mlp-5        [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "    Block-5       [[1, 291, 768]]       [1, 291, 768]            0       \n",
      " LayerNorm-11     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-21      [[1, 291, 768]]       [1, 291, 2304]       1,769,472   \n",
      "  Dropout-17    [[1, 12, 291, 291]]   [1, 12, 291, 291]          0       \n",
      "   Linear-22      [[1, 291, 768]]       [1, 291, 768]         590,592    \n",
      "  Dropout-18      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  Attention-6     [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  Identity-6      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      " LayerNorm-12     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-23      [[1, 291, 768]]       [1, 291, 3072]       2,362,368   \n",
      "    GELU-6        [[1, 291, 3072]]      [1, 291, 3072]           0       \n",
      "  Dropout-19      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "   Linear-24      [[1, 291, 3072]]      [1, 291, 768]        2,360,064   \n",
      "     Mlp-6        [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "    Block-6       [[1, 291, 768]]       [1, 291, 768]            0       \n",
      " LayerNorm-13     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-25      [[1, 291, 768]]       [1, 291, 2304]       1,769,472   \n",
      "  Dropout-20    [[1, 12, 291, 291]]   [1, 12, 291, 291]          0       \n",
      "   Linear-26      [[1, 291, 768]]       [1, 291, 768]         590,592    \n",
      "  Dropout-21      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  Attention-7     [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  Identity-7      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      " LayerNorm-14     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-27      [[1, 291, 768]]       [1, 291, 3072]       2,362,368   \n",
      "    GELU-7        [[1, 291, 3072]]      [1, 291, 3072]           0       \n",
      "  Dropout-22      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "   Linear-28      [[1, 291, 3072]]      [1, 291, 768]        2,360,064   \n",
      "     Mlp-7        [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "    Block-7       [[1, 291, 768]]       [1, 291, 768]            0       \n",
      " LayerNorm-15     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-29      [[1, 291, 768]]       [1, 291, 2304]       1,769,472   \n",
      "  Dropout-23    [[1, 12, 291, 291]]   [1, 12, 291, 291]          0       \n",
      "   Linear-30      [[1, 291, 768]]       [1, 291, 768]         590,592    \n",
      "  Dropout-24      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  Attention-8     [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  Identity-8      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      " LayerNorm-16     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-31      [[1, 291, 768]]       [1, 291, 3072]       2,362,368   \n",
      "    GELU-8        [[1, 291, 3072]]      [1, 291, 3072]           0       \n",
      "  Dropout-25      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "   Linear-32      [[1, 291, 3072]]      [1, 291, 768]        2,360,064   \n",
      "     Mlp-8        [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "    Block-8       [[1, 291, 768]]       [1, 291, 768]            0       \n",
      " LayerNorm-17     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-33      [[1, 291, 768]]       [1, 291, 2304]       1,769,472   \n",
      "  Dropout-26    [[1, 12, 291, 291]]   [1, 12, 291, 291]          0       \n",
      "   Linear-34      [[1, 291, 768]]       [1, 291, 768]         590,592    \n",
      "  Dropout-27      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  Attention-9     [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  Identity-9      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      " LayerNorm-18     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-35      [[1, 291, 768]]       [1, 291, 3072]       2,362,368   \n",
      "    GELU-9        [[1, 291, 3072]]      [1, 291, 3072]           0       \n",
      "  Dropout-28      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "   Linear-36      [[1, 291, 3072]]      [1, 291, 768]        2,360,064   \n",
      "     Mlp-9        [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "    Block-9       [[1, 291, 768]]       [1, 291, 768]            0       \n",
      " LayerNorm-19     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-37      [[1, 291, 768]]       [1, 291, 2304]       1,769,472   \n",
      "  Dropout-29    [[1, 12, 291, 291]]   [1, 12, 291, 291]          0       \n",
      "   Linear-38      [[1, 291, 768]]       [1, 291, 768]         590,592    \n",
      "  Dropout-30      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      " Attention-10     [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  Identity-10     [[1, 291, 768]]       [1, 291, 768]            0       \n",
      " LayerNorm-20     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-39      [[1, 291, 768]]       [1, 291, 3072]       2,362,368   \n",
      "    GELU-10       [[1, 291, 3072]]      [1, 291, 3072]           0       \n",
      "  Dropout-31      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "   Linear-40      [[1, 291, 3072]]      [1, 291, 768]        2,360,064   \n",
      "    Mlp-10        [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "   Block-10       [[1, 291, 768]]       [1, 291, 768]            0       \n",
      " LayerNorm-21     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-41      [[1, 291, 768]]       [1, 291, 2304]       1,769,472   \n",
      "  Dropout-32    [[1, 12, 291, 291]]   [1, 12, 291, 291]          0       \n",
      "   Linear-42      [[1, 291, 768]]       [1, 291, 768]         590,592    \n",
      "  Dropout-33      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      " Attention-11     [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  Identity-11     [[1, 291, 768]]       [1, 291, 768]            0       \n",
      " LayerNorm-22     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-43      [[1, 291, 768]]       [1, 291, 3072]       2,362,368   \n",
      "    GELU-11       [[1, 291, 3072]]      [1, 291, 3072]           0       \n",
      "  Dropout-34      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "   Linear-44      [[1, 291, 3072]]      [1, 291, 768]        2,360,064   \n",
      "    Mlp-11        [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "   Block-11       [[1, 291, 768]]       [1, 291, 768]            0       \n",
      " LayerNorm-23     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-45      [[1, 291, 768]]       [1, 291, 2304]       1,769,472   \n",
      "  Dropout-35    [[1, 12, 291, 291]]   [1, 12, 291, 291]          0       \n",
      "   Linear-46      [[1, 291, 768]]       [1, 291, 768]         590,592    \n",
      "  Dropout-36      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      " Attention-12     [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "  Identity-12     [[1, 291, 768]]       [1, 291, 768]            0       \n",
      " LayerNorm-24     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-47      [[1, 291, 768]]       [1, 291, 3072]       2,362,368   \n",
      "    GELU-12       [[1, 291, 3072]]      [1, 291, 3072]           0       \n",
      "  Dropout-37      [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "   Linear-48      [[1, 291, 3072]]      [1, 291, 768]        2,360,064   \n",
      "    Mlp-12        [[1, 291, 768]]       [1, 291, 768]            0       \n",
      "   Block-12       [[1, 291, 768]]       [1, 291, 768]            0       \n",
      " LayerNorm-25     [[1, 291, 768]]       [1, 291, 768]          1,536     \n",
      "   Linear-49         [[1, 768]]             [1, 2]             1,538     \n",
      "   Linear-50         [[1, 768]]             [1, 2]             1,538     \n",
      "===========================================================================\n",
      "Total params: 94,469,380\n",
      "Trainable params: 94,469,380\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n",
      "Input size (MB): 14.36\n",
      "Forward/backward pass size (MB): 529.51\n",
      "Params size (MB): 360.37\n",
      "Estimated Total Size (MB): 904.24\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "import paddle.nn as nn\r\n",
    "from ppim import DistilledVisionTransformer\r\n",
    "\r\n",
    "# 模型定义\r\n",
    "model = DistilledVisionTransformer(\r\n",
    "    img_size=1120,\r\n",
    "    patch_size=64,\r\n",
    "    class_dim=2)\r\n",
    "params = paddle.load('save_models/last.pdparams')\r\n",
    "model.set_state_dict(params)\r\n",
    "paddle.summary(model, (1, 3, 1120, 1120))\r\n",
    "model = paddle.Model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.2 开始训练\n",
    "这里分了两次跑，最开始就跑了100轮就很高的分数排到了第一，后来被JavaRoom大佬超过，就改成了第二种方式继续跑了50轮，直到最后val_loss都还在下降，我想就想跑下去会不会还能提高。也说不定了。\n",
    "1. CosineAnnealingDecay + Adam + bs64\n",
    "2. PolynomialDecay + SGD + ClipGradByGlobalNorm + bs8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous step.\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py:89: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if isinstance(slot[0], (np.ndarray, np.bool, numbers.Number)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  1/90 [..............................] - loss: 3.1512e-04 - acc: 1.0000 - ETA: 2:33 - 2s/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 90/90 [==============================] - loss: 0.0282 - acc: 0.9708 - 1s/step          \n",
      "save checkpoint at /home/aistudio/save_models/0\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/10 [==============================] - loss: 0.0122 - acc: 0.9875 - 867ms/step           \n",
      "Eval samples: 80\n",
      "Epoch 2/20\n",
      "step 90/90 [==============================] - loss: 0.0015 - acc: 0.9681 - 1s/step          \n",
      "Epoch 3/20\n",
      "step 90/90 [==============================] - loss: 0.0058 - acc: 0.9653 - 1s/step          \n",
      "Epoch 4/20\n",
      "step 90/90 [==============================] - loss: 0.0011 - acc: 0.9667 - 1s/step           \n",
      "Epoch 5/20\n",
      "step 90/90 [==============================] - loss: 9.6412e-04 - acc: 0.9653 - 1s/step    \n",
      "save checkpoint at /home/aistudio/save_models/4\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/10 [==============================] - loss: 0.0111 - acc: 1.0000 - 864ms/step           \n",
      "Eval samples: 80\n",
      "Epoch 6/20\n",
      "step 90/90 [==============================] - loss: 0.0026 - acc: 0.9681 - 1s/step        \n",
      "Epoch 7/20\n",
      "step 90/90 [==============================] - loss: 8.2225e-04 - acc: 0.9708 - 1s/step     \n",
      "Epoch 8/20\n",
      "step 90/90 [==============================] - loss: 0.5498 - acc: 0.9625 - 1s/step        \n",
      "Epoch 9/20\n",
      "step 90/90 [==============================] - loss: 0.0016 - acc: 0.9653 - 1s/step          \n",
      "save checkpoint at /home/aistudio/save_models/8\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/10 [==============================] - loss: 0.0107 - acc: 1.0000 - 861ms/step           \n",
      "Eval samples: 80\n",
      "Epoch 10/20\n",
      "step 90/90 [==============================] - loss: 0.0828 - acc: 0.9708 - 1s/step        \n",
      "Epoch 11/20\n",
      "step 90/90 [==============================] - loss: 0.0426 - acc: 0.9708 - 1s/step        \n",
      "Epoch 12/20\n",
      "step 90/90 [==============================] - loss: 5.0610e-04 - acc: 0.9625 - 1s/step     \n",
      "Epoch 13/20\n",
      "step 90/90 [==============================] - loss: 1.5559e-04 - acc: 0.9694 - 1s/step       \n",
      "save checkpoint at /home/aistudio/save_models/12\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/10 [==============================] - loss: 0.0106 - acc: 1.0000 - 865ms/step           \n",
      "Eval samples: 80\n",
      "Epoch 14/20\n",
      "step 90/90 [==============================] - loss: 0.0041 - acc: 0.9694 - 1s/step        \n",
      "Epoch 15/20\n",
      "step 90/90 [==============================] - loss: 0.0146 - acc: 0.9681 - 1s/step        \n",
      "Epoch 16/20\n",
      "step 90/90 [==============================] - loss: 0.0048 - acc: 0.9653 - 1s/step            \n",
      "Epoch 17/20\n",
      "step 90/90 [==============================] - loss: 0.0077 - acc: 0.9722 - 1s/step        \n",
      "save checkpoint at /home/aistudio/save_models/16\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/10 [==============================] - loss: 0.0102 - acc: 1.0000 - 884ms/step           \n",
      "Eval samples: 80\n",
      "Epoch 18/20\n",
      "step 90/90 [==============================] - loss: 8.6298e-04 - acc: 0.9722 - 1s/step      \n",
      "Epoch 19/20\n",
      "step 90/90 [==============================] - loss: 0.0714 - acc: 0.9764 - 1s/step            \n",
      "Epoch 20/20\n",
      "step 90/90 [==============================] - loss: 0.0120 - acc: 0.9722 - 1s/step          \n",
      "save checkpoint at /home/aistudio/save_models/final\n"
     ]
    }
   ],
   "source": [
    "# 模型准备\r\n",
    "# lr = paddle.optimizer.lr.CosineAnnealingDecay(learning_rate=3e-6, T_max=int(2*(800*0.9)), verbose=False)\r\n",
    "lr = paddle.optimizer.lr.PolynomialDecay(learning_rate=3e-7, decay_steps=1000)\r\n",
    "# opt = paddle.optimizer.Adam(learning_rate=lr, parameters=model.parameters(), weight_decay=paddle.regularizer.L2Decay(1e-7))\r\n",
    "opt = paddle.optimizer.SGD(learning_rate=lr, parameters=model.parameters(), \\\r\n",
    "                           weight_decay=paddle.regularizer.L2Decay(1e-9), grad_clip=paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0))\r\n",
    "# last\r\n",
    "opt_params = paddle.load('save_models/last.pdopt')\r\n",
    "opt.set_state_dict(opt_params)\r\n",
    "loss = nn.CrossEntropyLoss()\r\n",
    "metric = paddle.metric.Accuracy()\r\n",
    "model.prepare(optimizer=opt, loss=loss, metrics=metric)\r\n",
    "visualdl=paddle.callbacks.VisualDL(log_dir='visual_log')\r\n",
    "\r\n",
    "# 模型微调\r\n",
    "model.fit(\r\n",
    "    train_data=train_dataset, \r\n",
    "    eval_data=val_dataset, \r\n",
    "    batch_size=8, #  64, \r\n",
    "    epochs=50,  # 100, \r\n",
    "    eval_freq=4,  # 10, \r\n",
    "    log_freq=1, \r\n",
    "    save_dir='save_models', \r\n",
    "    save_freq=4,  # 10, \r\n",
    "    verbose=1, \r\n",
    "    drop_last=True,  # False, \r\n",
    "    shuffle=True,\r\n",
    "    num_workers=0,\r\n",
    "    callbacks=[visualdl]\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "val的acc已经无法看到啥有用的了，只能看loss是不是还在下降。\n",
    "```\n",
    "Eval begin...\n",
    "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
    "step 10/10 [==============================] - loss: 0.0102 - acc: 1.0000 - 884ms/step           \n",
    "Eval samples: 80\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. 模型预测\n",
    "- 预测这里就是主义图像的大小变了，而且相比于1444×1444，这个2124×2056还不是个正方形。最开始resize就是单边放到1120，结果老是不对，后来才反映过来。\n",
    "- 然后最坑的是结果必须升序排列（感谢吖吖查大佬的提醒），不然就只有0.5几分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T0314.jpg', 0.999534]\n",
      "['T0102.jpg', 0.99997854]\n",
      "['T0209.jpg', 0.9999782]\n",
      "['T0150.jpg', 0.99997663]\n",
      "['T0134.jpg', 0.99996257]\n",
      "['T0145.jpg', 0.99964404]\n",
      "['T0376.jpg', 0.00026754302]\n",
      "['T0084.jpg', 0.00033837082]\n",
      "['T0259.jpg', 8.749866e-05]\n",
      "['T0298.jpg', 0.0005694403]\n",
      "['T0239.jpg', 0.12634839]\n",
      "['T0230.jpg', 0.0008878632]\n",
      "['T0323.jpg', 0.00018469241]\n",
      "['T0012.jpg', 0.999982]\n",
      "['T0280.jpg', 0.0002507006]\n",
      "['T0015.jpg', 0.9972313]\n",
      "['T0141.jpg', 0.9694529]\n",
      "['T0064.jpg', 0.99994683]\n",
      "['T0233.jpg', 0.00037243208]\n",
      "['T0357.jpg', 0.00026328227]\n",
      "['T0124.jpg', 0.99963474]\n",
      "['T0043.jpg', 0.9981799]\n",
      "['T0214.jpg', 0.00022584207]\n",
      "['T0056.jpg', 0.0014357708]\n",
      "['T0126.jpg', 0.99997914]\n",
      "['T0049.jpg', 0.00021056297]\n",
      "['T0162.jpg', 0.00017513298]\n",
      "['T0306.jpg', 0.9999876]\n",
      "['T0385.jpg', 9.7773285e-05]\n",
      "['T0023.jpg', 0.9998442]\n",
      "['T0187.jpg', 0.00019756894]\n",
      "['T0343.jpg', 0.0002712552]\n",
      "['T0073.jpg', 0.9999851]\n",
      "['T0290.jpg', 0.00018723296]\n",
      "['T0289.jpg', 0.0051840045]\n",
      "['T0144.jpg', 0.00093522447]\n",
      "['T0155.jpg', 0.99938774]\n",
      "['T0099.jpg', 0.0020449688]\n",
      "['T0076.jpg', 0.00016188751]\n",
      "['T0384.jpg', 0.00028776078]\n",
      "['T0161.jpg', 0.0005136359]\n",
      "['T0287.jpg', 0.00029731003]\n",
      "['T0279.jpg', 0.9999778]\n",
      "['T0110.jpg', 0.99405885]\n",
      "['T0060.jpg', 0.0015521582]\n",
      "['T0059.jpg', 0.9999871]\n",
      "['T0010.jpg', 0.999984]\n",
      "['T0218.jpg', 0.9999703]\n",
      "['T0176.jpg', 0.9999875]\n",
      "['T0371.jpg', 0.9999176]\n",
      "['T0281.jpg', 0.0002640286]\n",
      "['T0324.jpg', 0.00044326726]\n",
      "['T0318.jpg', 0.00024267576]\n",
      "['T0198.jpg', 0.9999856]\n",
      "['T0108.jpg', 0.0007651617]\n",
      "['T0164.jpg', 0.0002141614]\n",
      "['T0082.jpg', 0.99998045]\n",
      "['T0136.jpg', 0.9999865]\n",
      "['T0050.jpg', 0.9999212]\n",
      "['T0035.jpg', 0.9989256]\n",
      "['T0163.jpg', 0.00041265844]\n",
      "['T0262.jpg', 0.00013323732]\n",
      "['T0054.jpg', 0.9999858]\n",
      "['T0129.jpg', 0.0002808684]\n",
      "['T0342.jpg', 0.99994576]\n",
      "['T0006.jpg', 0.00018106935]\n",
      "['T0197.jpg', 0.0002429818]\n",
      "['T0350.jpg', 0.9998766]\n",
      "['T0390.jpg', 0.00018347957]\n",
      "['T0083.jpg', 0.00015922738]\n",
      "['T0189.jpg', 0.00017516506]\n",
      "['T0399.jpg', 0.00011682774]\n",
      "['T0045.jpg', 0.00019984457]\n",
      "['T0339.jpg', 0.00012278097]\n",
      "['T0234.jpg', 0.012292903]\n",
      "['T0101.jpg', 0.4372792]\n",
      "['T0335.jpg', 0.00013887549]\n",
      "['T0270.jpg', 0.9999838]\n",
      "['T0039.jpg', 0.00014426111]\n",
      "['T0393.jpg', 0.99998116]\n",
      "['T0261.jpg', 0.9999521]\n",
      "['T0178.jpg', 0.99998724]\n",
      "['T0070.jpg', 0.0001965307]\n",
      "['T0308.jpg', 0.0006916987]\n",
      "['T0336.jpg', 0.015495885]\n",
      "['T0248.jpg', 0.00031046927]\n",
      "['T0132.jpg', 0.9251214]\n",
      "['T0153.jpg', 0.99630046]\n",
      "['T0041.jpg', 0.99883634]\n",
      "['T0128.jpg', 0.00018345227]\n",
      "['T0175.jpg', 0.99998105]\n",
      "['T0090.jpg', 0.00020591724]\n",
      "['T0302.jpg', 0.00046235163]\n",
      "['T0361.jpg', 0.00014904357]\n",
      "['T0283.jpg', 0.00012017337]\n",
      "['T0331.jpg', 0.00025293877]\n",
      "['T0098.jpg', 0.00015632839]\n",
      "['T0273.jpg', 0.9999846]\n",
      "['T0021.jpg', 0.0009207824]\n",
      "['T0203.jpg', 0.99998784]\n",
      "['T0316.jpg', 0.00029927635]\n",
      "['T0034.jpg', 0.9999833]\n",
      "['T0258.jpg', 0.00049632083]\n",
      "['T0242.jpg', 0.99998546]\n",
      "['T0352.jpg', 0.9999807]\n",
      "['T0188.jpg', 0.99998164]\n",
      "['T0231.jpg', 0.9999883]\n",
      "['T0249.jpg', 0.0002361876]\n",
      "['T0247.jpg', 0.00023699582]\n",
      "['T0338.jpg', 0.9999765]\n",
      "['T0268.jpg', 0.99998665]\n",
      "['T0031.jpg', 0.0001460162]\n",
      "['T0071.jpg', 0.99996614]\n",
      "['T0096.jpg', 0.00033267273]\n",
      "['T0344.jpg', 0.99998593]\n",
      "['T0387.jpg', 0.99998283]\n",
      "['T0008.jpg', 0.99998677]\n",
      "['T0382.jpg', 0.00014017613]\n",
      "['T0072.jpg', 0.94526577]\n",
      "['T0264.jpg', 0.99902177]\n",
      "['T0244.jpg', 0.999987]\n",
      "['T0253.jpg', 0.0022849035]\n",
      "['T0180.jpg', 0.999985]\n",
      "['T0058.jpg', 0.9999858]\n",
      "['T0080.jpg', 0.9995017]\n",
      "['T0395.jpg', 0.99998415]\n",
      "['T0282.jpg', 0.9999567]\n",
      "['T0312.jpg', 0.9984491]\n",
      "['T0329.jpg', 0.9999877]\n",
      "['T0036.jpg', 0.999984]\n",
      "['T0326.jpg', 0.99913317]\n",
      "['T0320.jpg', 0.9999862]\n",
      "['T0370.jpg', 0.9994337]\n",
      "['T0135.jpg', 0.0025446282]\n",
      "['T0394.jpg', 0.99998665]\n",
      "['T0119.jpg', 0.0001635991]\n",
      "['T0330.jpg', 0.00014222231]\n",
      "['T0095.jpg', 0.00041142956]\n",
      "['T0397.jpg', 0.9999858]\n",
      "['T0221.jpg', 0.9999794]\n",
      "['T0356.jpg', 0.9999877]\n",
      "['T0269.jpg', 0.00022201867]\n",
      "['T0305.jpg', 0.0003898446]\n",
      "['T0207.jpg', 0.9999869]\n",
      "['T0340.jpg', 0.9999865]\n",
      "['T0200.jpg', 0.0001139684]\n",
      "['T0115.jpg', 0.9999865]\n",
      "['T0360.jpg', 0.999974]\n",
      "['T0325.jpg', 0.999987]\n",
      "['T0210.jpg', 0.9999827]\n",
      "['T0048.jpg', 0.00021222542]\n",
      "['T0317.jpg', 0.0010364061]\n",
      "['T0029.jpg', 0.99998176]\n",
      "['T0003.jpg', 0.00011040601]\n",
      "['T0294.jpg', 0.00028830284]\n",
      "['T0169.jpg', 0.0004481686]\n",
      "['T0066.jpg', 0.99997485]\n",
      "['T0191.jpg', 0.0040284544]\n",
      "['T0181.jpg', 0.99998736]\n",
      "['T0179.jpg', 0.00020028945]\n",
      "['T0215.jpg', 0.000116223935]\n",
      "['T0120.jpg', 0.022818265]\n",
      "['T0375.jpg', 0.9998567]\n",
      "['T0078.jpg', 0.00019596954]\n",
      "['T0277.jpg', 0.00043027088]\n",
      "['T0377.jpg', 0.00018414416]\n",
      "['T0241.jpg', 0.9999882]\n",
      "['T0212.jpg', 0.00017081528]\n",
      "['T0068.jpg', 0.0009288073]\n",
      "['T0217.jpg', 0.00026930321]\n",
      "['T0014.jpg', 0.0029548388]\n",
      "['T0321.jpg', 0.9999856]\n",
      "['T0086.jpg', 0.99994075]\n",
      "['T0009.jpg', 0.99872667]\n",
      "['T0011.jpg', 0.9996723]\n",
      "['T0194.jpg', 0.99998677]\n",
      "['T0353.jpg', 0.9999865]\n",
      "['T0093.jpg', 0.00026869614]\n",
      "['T0044.jpg', 0.0002611933]\n",
      "['T0348.jpg', 0.00024891144]\n",
      "['T0285.jpg', 0.0007118358]\n",
      "['T0374.jpg', 0.99938726]\n",
      "['T0127.jpg', 0.9999423]\n",
      "['T0216.jpg', 0.00012061548]\n",
      "['T0202.jpg', 0.00017499262]\n",
      "['T0319.jpg', 0.99982977]\n",
      "['T0226.jpg', 0.00093859475]\n",
      "['T0177.jpg', 0.99998665]\n",
      "['T0372.jpg', 0.99997497]\n",
      "['T0199.jpg', 0.0002659259]\n",
      "['T0204.jpg', 0.00023829757]\n",
      "['T0067.jpg', 0.9987251]\n",
      "['T0237.jpg', 0.99990165]\n",
      "['T0088.jpg', 0.9999845]\n",
      "['T0142.jpg', 0.99997187]\n",
      "['T0275.jpg', 0.9999863]\n",
      "['T0065.jpg', 0.07221492]\n",
      "['T0379.jpg', 0.9999752]\n",
      "['T0347.jpg', 0.9999865]\n",
      "['T0307.jpg', 0.003265228]\n",
      "['T0121.jpg', 0.00025648554]\n",
      "['T0292.jpg', 0.00017889366]\n",
      "['T0057.jpg', 0.0027395955]\n",
      "['T0104.jpg', 0.00016919653]\n",
      "['T0388.jpg', 0.9999851]\n",
      "['T0055.jpg', 0.99965096]\n",
      "['T0196.jpg', 0.99998784]\n",
      "['T0170.jpg', 0.99987197]\n",
      "['T0332.jpg', 0.9998938]\n",
      "['T0061.jpg', 0.99997365]\n",
      "['T0299.jpg', 0.999987]\n",
      "['T0291.jpg', 0.00042011318]\n",
      "['T0125.jpg', 0.0006011187]\n",
      "['T0156.jpg', 9.709651e-05]\n",
      "['T0369.jpg', 0.99998724]\n",
      "['T0345.jpg', 0.999979]\n",
      "['T0190.jpg', 0.99998236]\n",
      "['T0019.jpg', 0.00036840077]\n",
      "['T0166.jpg', 0.99998593]\n",
      "['T0286.jpg', 0.0002361876]\n",
      "['T0182.jpg', 0.02546009]\n",
      "['T0396.jpg', 0.99996626]\n",
      "['T0103.jpg', 0.00032335756]\n",
      "['T0024.jpg', 0.00032105125]\n",
      "['T0272.jpg', 0.999982]\n",
      "['T0266.jpg', 0.999948]\n",
      "['T0105.jpg', 0.99992883]\n",
      "['T0118.jpg', 0.00024402796]\n",
      "['T0173.jpg', 0.99975735]\n",
      "['T0085.jpg', 0.9997948]\n",
      "['T0256.jpg', 0.998536]\n",
      "['T0359.jpg', 0.9999846]\n",
      "['T0140.jpg', 0.037378978]\n",
      "['T0184.jpg', 0.00029681952]\n",
      "['T0075.jpg', 0.9998623]\n",
      "['T0389.jpg', 0.00013947985]\n",
      "['T0137.jpg', 0.06930005]\n",
      "['T0143.jpg', 0.99998736]\n",
      "['T0097.jpg', 0.9999858]\n",
      "['T0309.jpg', 0.0003620935]\n",
      "['T0303.jpg', 0.99996984]\n",
      "['T0038.jpg', 0.9023473]\n",
      "['T0051.jpg', 0.9997687]\n",
      "['T0211.jpg', 0.99998534]\n",
      "['T0025.jpg', 0.0004123012]\n",
      "['T0107.jpg', 0.99998724]\n",
      "['T0263.jpg', 0.9999858]\n",
      "['T0157.jpg', 9.839348e-05]\n",
      "['T0245.jpg', 0.0001377786]\n",
      "['T0225.jpg', 0.9999826]\n",
      "['T0250.jpg', 0.9999869]\n",
      "['T0020.jpg', 0.99746287]\n",
      "['T0037.jpg', 0.9999844]\n",
      "['T0251.jpg', 0.99998534]\n",
      "['T0296.jpg', 0.000261146]\n",
      "['T0310.jpg', 0.014864465]\n",
      "['T0026.jpg', 0.99998665]\n",
      "['T0185.jpg', 0.00020879964]\n",
      "['T0042.jpg', 0.000116267394]\n",
      "['T0186.jpg', 0.00015530574]\n",
      "['T0158.jpg', 0.99998]\n",
      "['T0243.jpg', 0.00048512968]\n",
      "['T0227.jpg', 0.99980825]\n",
      "['T0152.jpg', 0.04944566]\n",
      "['T0168.jpg', 0.00031282305]\n",
      "['T0346.jpg', 0.9999845]\n",
      "['T0114.jpg', 0.9999862]\n",
      "['T0122.jpg', 0.9999348]\n",
      "['T0358.jpg', 0.00010299187]\n",
      "['T0333.jpg', 0.00017603605]\n",
      "['T0220.jpg', 0.9970163]\n",
      "['T0311.jpg', 0.9999871]\n",
      "['T0123.jpg', 0.99998164]\n",
      "['T0365.jpg', 0.00013929137]\n",
      "['T0354.jpg', 0.9999845]\n",
      "['T0149.jpg', 0.000129151]\n",
      "['T0366.jpg', 0.0020192985]\n",
      "['T0159.jpg', 0.99976295]\n",
      "['T0367.jpg', 0.99998486]\n",
      "['T0328.jpg', 0.00021871367]\n",
      "['T0046.jpg', 0.00038400094]\n",
      "['T0400.jpg', 0.9939872]\n",
      "['T0167.jpg', 0.9999851]\n",
      "['T0362.jpg', 0.025035143]\n",
      "['T0004.jpg', 0.47204617]\n",
      "['T0131.jpg', 0.9998504]\n",
      "['T0267.jpg', 0.00017195592]\n",
      "['T0246.jpg', 0.99998736]\n",
      "['T0081.jpg', 0.9989698]\n",
      "['T0030.jpg', 0.0002888757]\n",
      "['T0106.jpg', 0.8058862]\n",
      "['T0116.jpg', 0.9999865]\n",
      "['T0028.jpg', 0.00017056106]\n",
      "['T0171.jpg', 0.00026355023]\n",
      "['T0219.jpg', 0.056772877]\n",
      "['T0206.jpg', 0.9979152]\n",
      "['T0092.jpg', 0.99998546]\n",
      "['T0381.jpg', 0.00024446554]\n",
      "['T0363.jpg', 0.0003481748]\n",
      "['T0002.jpg', 0.00029589675]\n",
      "['T0063.jpg', 0.00025510686]\n",
      "['T0213.jpg', 0.99998295]\n",
      "['T0391.jpg', 0.00029015253]\n",
      "['T0224.jpg', 0.1605943]\n",
      "['T0229.jpg', 0.00017103953]\n",
      "['T0383.jpg', 0.0005832321]\n",
      "['T0130.jpg', 0.9999865]\n",
      "['T0133.jpg', 0.9999863]\n",
      "['T0112.jpg', 0.00014396153]\n",
      "['T0154.jpg', 0.99998665]\n",
      "['T0111.jpg', 0.021369265]\n",
      "['T0236.jpg', 0.3026058]\n",
      "['T0005.jpg', 0.9999614]\n",
      "['T0139.jpg', 0.99992096]\n",
      "['T0089.jpg', 0.00032086583]\n",
      "['T0027.jpg', 0.0002074119]\n",
      "['T0222.jpg', 0.9999864]\n",
      "['T0195.jpg', 0.00011930367]\n",
      "['T0007.jpg', 0.9999801]\n",
      "['T0147.jpg', 0.00022478469]\n",
      "['T0053.jpg', 0.0006530966]\n",
      "['T0315.jpg', 0.9999876]\n",
      "['T0380.jpg', 0.9999865]\n",
      "['T0300.jpg', 0.99998426]\n",
      "['T0016.jpg', 0.00012924586]\n",
      "['T0148.jpg', 0.99974626]\n",
      "['T0257.jpg', 0.0003395354]\n",
      "['T0228.jpg', 0.00039075734]\n",
      "['T0260.jpg', 0.99998546]\n",
      "['T0151.jpg', 0.9999877]\n",
      "['T0192.jpg', 0.00078377814]\n",
      "['T0033.jpg', 0.00016436762]\n",
      "['T0232.jpg', 0.96666]\n",
      "['T0255.jpg', 0.00017205795]\n",
      "['T0355.jpg', 0.009106031]\n",
      "['T0193.jpg', 0.00019146544]\n",
      "['T0284.jpg', 0.00022931685]\n",
      "['T0373.jpg', 0.99998415]\n",
      "['T0386.jpg', 0.00026142824]\n",
      "['T0337.jpg', 0.00018347957]\n",
      "['T0252.jpg', 0.00072831265]\n",
      "['T0238.jpg', 0.9999604]\n",
      "['T0364.jpg', 0.9989492]\n",
      "['T0351.jpg', 0.99998665]\n",
      "['T0165.jpg', 0.00013044215]\n",
      "['T0069.jpg', 0.9999852]\n",
      "['T0301.jpg', 0.0021121027]\n",
      "['T0265.jpg', 0.020331971]\n",
      "['T0334.jpg', 0.00017556119]\n",
      "['T0174.jpg', 0.999987]\n",
      "['T0077.jpg', 0.00016617167]\n",
      "['T0398.jpg', 0.9999871]\n",
      "['T0293.jpg', 0.00032671224]\n",
      "['T0288.jpg', 0.9999865]\n",
      "['T0094.jpg', 0.0018386793]\n",
      "['T0349.jpg', 0.99818546]\n",
      "['T0223.jpg', 0.9999875]\n",
      "['T0240.jpg', 0.99998724]\n",
      "['T0172.jpg', 0.99998665]\n",
      "['T0022.jpg', 0.00016649062]\n",
      "['T0274.jpg', 0.00036574338]\n",
      "['T0160.jpg', 0.99998736]\n",
      "['T0295.jpg', 0.00015720734]\n",
      "['T0052.jpg', 0.00016728594]\n",
      "['T0017.jpg', 0.0004417548]\n",
      "['T0040.jpg', 0.00016731785]\n",
      "['T0100.jpg', 0.9999269]\n",
      "['T0235.jpg', 0.56284267]\n",
      "['T0183.jpg', 0.99995327]\n",
      "['T0079.jpg', 0.999979]\n",
      "['T0327.jpg', 0.9764854]\n",
      "['T0001.jpg', 0.9999871]\n",
      "['T0113.jpg', 0.99998486]\n",
      "['T0018.jpg', 0.9999869]\n",
      "['T0032.jpg', 0.00022240893]\n",
      "['T0392.jpg', 0.99509627]\n",
      "['T0313.jpg', 0.00017470685]\n",
      "['T0378.jpg', 0.99989367]\n",
      "['T0368.jpg', 0.99998057]\n",
      "['T0109.jpg', 0.9999827]\n",
      "['T0201.jpg', 0.7439287]\n",
      "['T0278.jpg', 0.9999825]\n",
      "['T0322.jpg', 0.00045209363]\n",
      "['T0271.jpg', 0.9856436]\n",
      "['T0208.jpg', 0.99998367]\n",
      "['T0087.jpg', 0.9998247]\n",
      "['T0013.jpg', 0.9993368]\n",
      "['T0062.jpg', 0.17861645]\n",
      "['T0341.jpg', 0.99997926]\n",
      "['T0276.jpg', 0.9999863]\n",
      "['T0138.jpg', 0.00015054084]\n",
      "['T0146.jpg', 0.023511697]\n",
      "['T0205.jpg', 0.0013170734]\n",
      "['T0074.jpg', 0.9999262]\n",
      "['T0091.jpg', 0.9999846]\n",
      "['T0304.jpg', 0.00019410466]\n",
      "['T0047.jpg', 0.0003473218]\n",
      "['T0254.jpg', 0.00031346158]\n",
      "['T0297.jpg', 0.07354774]\n",
      "['T0117.jpg', 0.0017190423]\n"
     ]
    }
   ],
   "source": [
    "import os\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from PIL import Image\r\n",
    "import paddle.vision.transforms as T\r\n",
    "import paddle\r\n",
    "import paddle.nn as nn\r\n",
    "import paddle.nn.functional as F\r\n",
    "from ppim import DistilledVisionTransformer\r\n",
    "\r\n",
    "save_path = 'Classification_Results.csv'\r\n",
    "file_path = 'PLAM/PALM-Testing400-Images'\r\n",
    "imgs_name = os.listdir(file_path)\r\n",
    "\r\n",
    "model = DistilledVisionTransformer(\r\n",
    "    img_size=1120,\r\n",
    "    patch_size=64,\r\n",
    "    class_dim=2)\r\n",
    "params = paddle.load('save_models/last.pdparams')\r\n",
    "model.set_state_dict(params)\r\n",
    "model.eval()\r\n",
    "\r\n",
    "inf_transforms = T.Compose([\r\n",
    "    T.Resize((1120, 1120), interpolation='bicubic'),  # 1120X1120\r\n",
    "    T.ToTensor()\r\n",
    "])\r\n",
    "\r\n",
    "pre_data = []\r\n",
    "for img_name in imgs_name:\r\n",
    "    data_path = os.path.join(file_path, img_name)\r\n",
    "    data = np.asarray(Image.open(data_path).convert('RGB'))\r\n",
    "    data = inf_transforms(data)\r\n",
    "    data = data.astype('float32').reshape([1, 3, 1120, 1120])\r\n",
    "    pre = model(data)\r\n",
    "    pre = F.softmax(pre)\r\n",
    "    print([img_name, pre.numpy()[0][1]])\r\n",
    "    pre_data.append([img_name, pre.numpy()[0][1]])\r\n",
    "\r\n",
    "sorted(pre_data)  # 升序（我这是后面写文字加的，所以看到输出没有排序）\r\n",
    "\r\n",
    "df = pd.DataFrame(pre_data, columns=['FileName', 'PM Risk'])\r\n",
    "df.to_csv(save_path, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 心得\n",
    "1. 个人感觉VisionTransformer在医学里面的效果真的牛逼，啥技巧也不会真的就靠这就能上分。\n",
    "2. 果然Adam开始牛逼，SGD最后调整，效果都挺好的。\n",
    "3. AI Studio太卷了，这个月要完了大佬们就开始疯狂涨分。\n",
    "\n",
    "## *参考资料\n",
    "1. [PPIM](https://github.com/AgentMaker/Paddle-Image-Models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
